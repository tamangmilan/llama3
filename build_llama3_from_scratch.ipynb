{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZNO7Wob-uUZk"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1: Input Block ###\n",
        "\n",
        "# Using Tiny Shakespeare dataset for character-level tokenizer. Some part of the following character-level tokenizer is referenced from Andrej karpathy's GitHub (https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py) which I found is explained very well.\n",
        "# Load tiny_shakespeare data file (https://github.com/tamangmilan/llama3/blob/main/tiny_shakespeare.txt)\n",
        "\n",
        "device: str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availability\n",
        "\n",
        "# Load tiny_shakespeare data file.\n",
        "with open('tiny_shakespeare.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# Prepare vocabulary by taking all the unique characters from the tiny_shakespeare data\n",
        "vocab = sorted(list(set(data)))\n",
        "\n",
        "# Training Llama 3 model requires addtional tokens such as <|begin_of_text|>, <|end_of_text|> and <|pad_id|>, we'll add them into vocabulary\n",
        "vocab.extend(['<|begin_of_text|>','<|end_of_text|>','<|pad_id|>'])\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create a mapping between characters with corresponding integer indexes in vocabulary.\n",
        "# This is important to build tokenizers encode and decode functions.\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "\n",
        "# Tokenizers encode function: take a string, output a list of integers\n",
        "def encode(s):\n",
        "  return [stoi[ch] for ch in s]\n",
        "\n",
        "# Tokenizers decode function: take a list of integers, output a string\n",
        "def decode(l):\n",
        "  return ''.join(itos[i] for i in l)\n",
        "\n",
        "# Define tensor token variable to be used later during model training\n",
        "token_bos = torch.tensor([stoi['<|begin_of_text|>']], dtype=torch.int, device=device)\n",
        "token_eos = torch.tensor([stoi['<|end_of_text|>']], dtype=torch.int, device=device)\n",
        "token_pad = torch.tensor([stoi['<|pad_id|>']], dtype=torch.int, device=device)\n",
        "\n",
        "prompts = \"Hello World\"\n",
        "encoded_tokens = encode(prompts)\n",
        "decoded_text = decode(encoded_tokens)\n",
        "\n",
        "### Test: Input Block Code ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "print(f\"Lenth of shakespeare in character: {len(data)}\")\n",
        "print(f\"The vocabulary looks like this: {''.join(vocab)}\\n\")\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"encoded_tokens: {encoded_tokens}\")\n",
        "print(f\"decoded_text: {decoded_text}\")\n",
        "\"\"\"\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "Lenth of shakespeare in character: 1115394\n",
        "The vocabulary looks like this:\n",
        " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<|begin_of_text|><|end_of_text|><|pad_id|>\n",
        "\n",
        "Vocab size: 68\n",
        "encoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
        "decoded_text: Hello World\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "JLQBvREzuVx0",
        "outputId": "8aeac753-3f11-4e71-f760-3043a07af153"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nLenth of shakespeare in character: 1115394\\nThe vocabulary looks like this: \\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<|begin_of_text|><|end_of_text|><|pad_id|>\\n\\nVocab size: 68\\nencoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\\ndecoded_text: Hello World\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step2: The Decoder Block\n",
        "# Note: Since the Llama 3 model is developed by Meta, so to be in sync with their codebase and for future compatibility,\n",
        "# I will use most of the code from Meta GitHub with some necessary changes required to achieve our goal.\n",
        "\n",
        "# Define parameters dataclass: we'll use these parameters during model building, training and inference.\n",
        "# Note: Since we want to see the results of training and inferencing faster rather than focusing on high accuracy, we're taking lower values for most of the parameters which are set higher in the Llama 3 model.\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 512              # embedding dimension\n",
        "    n_layers: int = 8           # number of model decoder blocks\n",
        "    n_heads: int = 8            # number of heads for queries embedding\n",
        "    n_kv_heads: int = 4         # number of heads for keys and values embedding\n",
        "    vocab_size: int = len(vocab) # Length of vocabulary\n",
        "    multiple_of: int = 256        # Require to calculate dim of feedfoward network\n",
        "    ffn_dim_multiplier: Optional[float] = None  # Require to calculate dim of feedfoward network\n",
        "    norm_eps: float = 1e-5                       # Default Epsilon value set for the RMSNorm calculation\n",
        "    rope_theta: float = 10000.0   # Default theta value for the RePE calculation\n",
        "\n",
        "    max_batch_size: int = 10      # Max batch size\n",
        "    max_seq_len: int = 256         # Max sequence length\n",
        "\n",
        "    epochs: int = 2500             # Total number of training iteration\n",
        "    log_interval: int = 10        # Number of interval to print the logs and loss values\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availability"
      ],
      "metadata": {
        "id": "pl1W0ndTuV0A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step2a: The RMSNorm\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim: int, eps: float = 1e-6):\n",
        "    super().__init__()\n",
        "    device = ModelArgs.device\n",
        "    self.eps = eps\n",
        "    # Scaling parameter gamma, initialized with one and the no of parameters is equal to the size of dim\n",
        "    self.weight = nn.Parameter(torch.ones(dim).to(device))\n",
        "\n",
        "  def _norm(self, x):\n",
        "    return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #Shape: x[bs,seq,dim]\n",
        "    output = self._norm(x.float()).type_as(x)\n",
        "\n",
        "    #Shape: x[bs,seq,dim] -> x_norm[bs,seq,dim]\n",
        "    return output * self.weight\n",
        "\n",
        "### Test: RMSNorm Code ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)\n",
        "rms_norm = RMSNorm(dim=ModelArgs.dim)\n",
        "x_norm = rms_norm(x)\n",
        "\n",
        "print(f\"Shape of x: {x.shape}\")\n",
        "print(f\"Shape of x_norm: {x_norm.shape}\")\n",
        "\"\"\"\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "Shape of x: torch.Size([10, 256, 512])\n",
        "Shape of x_norm: torch.Size([10, 256, 512])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cCEirGpxuV2N",
        "outputId": "d764163b-7024-4723-c8bc-98a9f328c276"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nShape of x: torch.Size([10, 256, 512])\\nShape of x_norm: torch.Size([10, 256, 512])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step2b: The RoPE\n",
        "def precompute_freqs_cis(dim:int, seq_len: int, theta: float=10000.0):\n",
        "  # Computing Theta value for each dim pair which is dim/2\n",
        "  device = ModelArgs.device\n",
        "  freqs = 1.0 / (theta ** (torch.arange(0, dim, 2,device=device)[:(dim//2)].float()/dim))\n",
        "\n",
        "  # Computing range of positions(m) in the sequence\n",
        "  t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
        "\n",
        "  # freqs gives all the Theta value range for all the position of tokens in the sequence\n",
        "  freqs = torch.outer(t, freqs).to(device)\n",
        "\n",
        "  # This is the rotation matrix which needs to be converted to Polar form in order to perform rotation to the embedding\n",
        "  freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)\n",
        "  return freqs_cis\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis, x):\n",
        "  ndim = x.ndim\n",
        "  assert 0<=1<ndim\n",
        "  assert freqs_cis.shape == (x.shape[1],x.shape[-1]), \"the last two dimension of freqs_cis, x must match\"\n",
        "  shape = [d if i==1 or i==ndim-1 else 1 for i,d in enumerate(x.shape)]\n",
        "  return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]:\n",
        "  device = ModelArgs.device\n",
        "  # Applying rotary positional encoding to both query and key embedding together\n",
        "  # First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.\n",
        "  # Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number\n",
        "  xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)).to(device)    #xq_:[bsz, seq_len, n_heads, head_dim/2]\n",
        "  xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)).to(device)    #xk_:[bsz, seq_len, n_heads, head_dim/2]\n",
        "\n",
        "  # The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding\n",
        "  # Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis:[seq_len,head_dim] -> freqs_cis:[1,seq_len,1,head_dim]\n",
        "  freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "\n",
        "  #Finally, perform rotation operation by multiplying with freqs_cis.\n",
        "  #After the rotation is completed, convert both xq_out and xk_out back to real number and return\n",
        "  xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).to(device) #xq_out:[bsz, seq_len, n_heads, head_dim]\n",
        "  xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).to(device) #xk_out:[bsz, seq_len, n_heads, head_dim]\n",
        "  return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "### Test: RoPE Code ###\n",
        "# Note: x_norm is calculated during RMSNorm and is being used for testing here.\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "head_dim = ModelArgs.dim//ModelArgs.n_heads\n",
        "wq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=device)\n",
        "wk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=device)\n",
        "xq = wq(x_norm)\n",
        "xk = wk(x_norm)\n",
        "print(f\"xq.shape: {xq.shape}\")\n",
        "print(f\"xk.shape: {xk.shape}\")\n",
        "\n",
        "xq = xq.view(xq.shape[0],xq.shape[1],ModelArgs.n_heads, head_dim)\n",
        "xk = xk.view(xk.shape[0],xk.shape[1],ModelArgs.n_kv_heads, head_dim)\n",
        "print(f\"xq.re-shape: {xq.shape}\")\n",
        "print(f\"xk.re-shape: {xk.shape}\")\n",
        "\n",
        "freqs_cis = precompute_freqs_cis(dim=head_dim, seq_len=ModelArgs.max_seq_len)\n",
        "print(f\"freqs_cis.shape: {freqs_cis.shape}\")\n",
        "\n",
        "xq_rotate, xk_rotate = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "print(f\"xq_rotate.shape: {xq_rotate.shape}\")\n",
        "print(f\"xk_rotate.shape: {xk_rotate.shape}\")\n",
        "\"\"\"\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "xq.shape: torch.Size([10, 256, 512])\n",
        "xk.shape: torch.Size([10, 256, 256])\n",
        "xq.re-shape: torch.Size([10, 256, 8, 64])\n",
        "xk.re-shape: torch.Size([10, 256, 4, 64])\n",
        "freqs_cis.shape: torch.Size([256, 32])\n",
        "xq_rotate.shape: torch.Size([10, 256, 8, 64])\n",
        "xk_rotate.shape: torch.Size([10, 256, 4, 64])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "H0KYg5dluV4E",
        "outputId": "294d9471-cf70-4bb0-aa79-8c5467127762"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nxq.shape: torch.Size([10, 256, 512])\\nxk.shape: torch.Size([10, 256, 256])\\nxq.re-shape: torch.Size([10, 256, 8, 64])\\nxk.re-shape: torch.Size([10, 256, 4, 64])\\nfreqs_cis.shape: torch.Size([256, 32])\\nxq_rotate.shape: torch.Size([10, 256, 8, 64])\\nxk_rotate.shape: torch.Size([10, 256, 4, 64])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The Attention Block [Step2c: The KV Cache; Step2d: Group Query Attention]\n",
        "## As mentioned before, the naming convention follows original the meta's LLama3 GitHub\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    self.args = args\n",
        "    # Embedding dimension\n",
        "    self.dim = args.dim\n",
        "    # Number of heads assigned to Query\n",
        "    self.n_heads = args.n_heads\n",
        "    # Number of heads assigned to Key and values. If \"None\", the number will be same as Query.\n",
        "    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    # Dimension of each head relative to model dimension\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "    # Number of repetition in order to make time Key, Value heads to match Query heads number\n",
        "    self.n_rep = args.n_heads // args.n_kv_heads\n",
        "\n",
        "    # Weight initialize for Keys, Querys, Values and Oupt. Notice that the out_feature value of weight for q and kv are based on it's heads\n",
        "    self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False, device=device)\n",
        "    self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False, device=device)\n",
        "    self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False, device=device)\n",
        "    self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=False, device=device)\n",
        "\n",
        "    # Initialize caches to store Key, Values at start. (KV Cache Implementation)\n",
        "    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n",
        "    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos, inference):\n",
        "    # Shape of the input embedding: [bsz,seq_len,dim]\n",
        "    bsz, seq_len, _ = x.shape\n",
        "    # Mask will be used during 'Training' and is not required for 'inference' due to the use of KV cache.\n",
        "    mask = None\n",
        "\n",
        "    xq = self.wq(x)  #x[bsz,seq_len,dim]*wq[dim,n_heads * head_dim] -> q[bsz,seq_len,n_heads * head_dim]\n",
        "    xk = self.wk(x)  #x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -> k[bsz,seq_len,n_kv_heads * head_dim]\n",
        "    xv = self.wv(x)  #x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -> v[bsz,seq_len,n_kv_heads * head_dim]\n",
        "\n",
        "    # Reshaping Querys, Keys and Values by their number of heads. (Group Query Attention Implementation)\n",
        "    xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)      #xq[bsz,seq_len,n_heads, head_dim]\n",
        "    xk = xk.view(bsz, seq_len, self.n_kv_heads, self.head_dim)   #xk[bsz,seq_len,n_kv_heads, head_dim]\n",
        "    xv = xv.view(bsz, seq_len, self.n_kv_heads, self.head_dim)   #xv[bsz,seq_len,n_kv_heads, head_dim]\n",
        "\n",
        "    # Model - Inference Mode: kv-cache is enabled at inference mode only.\n",
        "    if inference:\n",
        "      # Compute rotation matrix for each position in the sequence\n",
        "      freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len * 2)\n",
        "      # During inferencing, we should only take the rotation matrix range from the current position of the tokens.\n",
        "      freqs_cis = freqs_cis[start_pos : start_pos + seq_len]\n",
        "      # Apply RoPE to Queries and Keys embeddings\n",
        "      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "\n",
        "      self.cache_k = self.cache_k.to(xq)\n",
        "      self.cache_v = self.cache_v.to(xq)\n",
        "      # Store Keys and Values token embedding into their respective cache [KV Cache Implementation]\n",
        "      self.cache_k[:bsz, start_pos:start_pos + seq_len] = xk\n",
        "      self.cache_v[:bsz, start_pos:start_pos + seq_len] = xv\n",
        "\n",
        "      # Assign all the previous tokens embeddings upto current tokens position to Keys and Values variable for Attention Calculation\n",
        "      keys = self.cache_k[:bsz, :start_pos + seq_len]\n",
        "      values = self.cache_v[:bsz, :start_pos + seq_len]\n",
        "\n",
        "      # At this point, they Keys and Values shape aren't same with Queries Embedding which has to be in order to computer attention score\n",
        "      # Use repeat_kv function to make Keys,Values shape same as queries shape\n",
        "      keys = repeat_kv(keys, self.n_rep)      #keys[bsz,seq_len,n_heads,head_dim]\n",
        "      values = repeat_kv(values, self.n_rep)  #values[bsz,seq_len,n_heads,head_dim]\n",
        "\n",
        "    # Mode - Training mode: KV-Cache not implemented\n",
        "    else:\n",
        "      # Compute rotation matrix and apply RoPE to queries and keys for for training.\n",
        "      freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len)\n",
        "\n",
        "      #xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]\n",
        "      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "\n",
        "      # Use repeat_kv function to make Keys,Values shape same as the queries shape\n",
        "      #keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]\n",
        "      keys = repeat_kv(xk, self.n_rep)\n",
        "      values = repeat_kv(xv, self.n_rep)\n",
        "\n",
        "      # For training mode, we'll compute mask and apply to the attention score later\n",
        "      mask = torch.full((seq_len, seq_len),float(\"-inf\"),device=self.args.device)\n",
        "      mask = torch.triu(mask, diagonal=1).to(self.args.device)\n",
        "\n",
        "    # To compute attention, we'll need to perform a transpose operation to reshape all queries, keys and values bring heads at dim 1 and seq at dim 2\n",
        "    xq = xq.transpose(1,2)                  #xq[bsz,n_heads,seq_len,head_dim]\n",
        "    keys = keys.transpose(1,2)              #keys[bsz,n_heads,seq_len,head_dim]\n",
        "    values = values.transpose(1,2)          #values[bsz,n_heads,seq_len,head_dim]\n",
        "\n",
        "    # Computing attention score\n",
        "    scores = torch.matmul(xq, keys.transpose(2,3)).to(self.args.device)/math.sqrt(self.head_dim)\n",
        "    if mask is not None:\n",
        "      scores = scores + mask\n",
        "\n",
        "    # Apply softmax to the attention score\n",
        "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "    # Matrix multiplication of attention score with the values\n",
        "    output = torch.matmul(scores, values).to(self.args.device)\n",
        "\n",
        "    # We get the contextual embedding for each head\n",
        "    # All heads need to be reshaped back and combined to give a single single contextual attention output\n",
        "    # Shape change: output[bsz,n_heads,seq_len,head_dim] -> output[bsz,seq_len, n_heads,head_dim] -> output[bsz,seq_len, n_heads * head_dim]\n",
        "    output = output.transpose(1,2).contiguous().view(bsz, seq_len, -1)\n",
        "\n",
        "    # shape: output [bsz,seq_len,dim]\n",
        "    return self.wo(output)\n",
        "\n",
        "# If the number of keys/values heads is less than query heads, this function expands the key/values embeddings with the required number of repetition\n",
        "def repeat_kv(x:torch.Tensor, n_rep: int)-> torch.Tensor:\n",
        "  bsz, seq_len, n_kv_heads, head_dim = x.shape\n",
        "  if n_rep == 1:\n",
        "    return x\n",
        "  return (\n",
        "      x[:,:,:,None,:]\n",
        "      .expand(bsz,seq_len,n_kv_heads,n_rep, head_dim)\n",
        "      .reshape(bsz,seq_len,n_kv_heads * n_rep, head_dim)\n",
        "  )\n",
        "\n",
        "\n",
        "### Test: Repeat_kv function ###\n",
        "# note: xk, x_norm is already calculated during RoPE, RMSNorm testing and is being used for testing here.\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "n_rep = ModelArgs.n_heads // ModelArgs.n_kv_heads\n",
        "keys = repeat_kv(xk, n_rep)\n",
        "print(f\"xk.shape: {xk.shape}\")\n",
        "print(f\"keys.shape: {keys.shape}\")\n",
        "\n",
        "## Test: Attention function\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\n",
        "attention = Attention(ModelArgs)\n",
        "x_out = attention(x_norm,start_pos=0, inference=False)\n",
        "print(f\"x_out.shape: {x_out.shape}\")\n",
        "\"\"\"\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "xk.shape: torch.Size([10, 256, 4, 64])\n",
        "keys.shape: torch.Size([10, 256, 8, 64])\n",
        "x_out.shape: torch.Size([10, 256, 512])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NuwviqiauV6Y",
        "outputId": "eac4fbf8-51f4-4cd5-f6d6-172b74720bf0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nxk.shape: torch.Size([10, 256, 4, 64])\\nkeys.shape: torch.Size([10, 256, 8, 64])\\nx_out.shape: torch.Size([10, 256, 512])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step2e: The Feedfoward Network (SwiGLU activation)\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim:int, hidden_dim:int, multiple_of:int, ffn_dim_multiplier: Optional[float]):\n",
        "    super().__init__()\n",
        "    # Models embedding dimension\n",
        "    self.dim = dim\n",
        "\n",
        "    # We must use the hidden dimensions calculation shared by Meta which is the ideal one for this model\n",
        "    # Hidden dimension are calculated such that it is a multiple of 256.\n",
        "    hidden_dim = int(2 * hidden_dim/3)\n",
        "    if ffn_dim_multiplier is not None:\n",
        "      hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "    # define hiddne layers weights\n",
        "    self.w1 = nn.Linear(self.dim, hidden_dim, bias=False, device=device)\n",
        "    self.w2 = nn.Linear(hidden_dim, self.dim, bias=False, device=device)\n",
        "    self.w3 = nn.Linear(self.dim, hidden_dim, bias=False, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Shape: [bsz,seq_len,dim]\n",
        "    return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "\n",
        "### Test: FeedForward module ###\n",
        "# note: x_out is already computed at Attention testing and is being used for testing here.\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "feed_forward = FeedForward(ModelArgs.dim, 4 * ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)\n",
        "x_out = rms_norm(x_out)\n",
        "x_out = feed_forward(x_out)\n",
        "print(f\"feed forward output: x_out.shape: {x_out.shape}\")\n",
        "\"\"\"\n",
        "\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "feed forward output: x_out.shape: torch.Size([10, 256, 512])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rTlX6GB8uV8d",
        "outputId": "d5affd46-ebe0-4139-8b4e-55009b111207"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfeed forward output: x_out.shape: torch.Size([10, 256, 512])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step2f: The Decoder Block. The class name is assigned as TransformerBlock to match the name of Meta llama 3 code base.\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    self.args = args\n",
        "    # Initilizate RMSNorm for attention\n",
        "    self.attention_norm = RMSNorm(dim=args.dim, eps = args.norm_eps)\n",
        "    # Initilizate Attention class\n",
        "    self.attention = Attention(args)\n",
        "    # Initilizate RMSNorm for feedfoward class\n",
        "    self.ff_norm = RMSNorm(dim=args.dim, eps = args.norm_eps)\n",
        "    # Initilizate feedfoward class\n",
        "    self.feedforward = FeedForward(args.dim, 4 * args.dim, args.multiple_of, args.ffn_dim_multiplier)\n",
        "\n",
        "  def forward(self, x, start_pos, inference):\n",
        "    # start_pos = token position for inference mode, inference = True for inference and False for training mode\n",
        "    # i) pass input embedding to attention_norm and then pass to attention block.\n",
        "    # ii) the output of attention is then added to embedding(before norm)\n",
        "    h = x + self.attention(self.attention_norm(x), start_pos, inference)\n",
        "\n",
        "    # i) pass attention output to ff_norm and then pass to the feedforward network.\n",
        "    # ii) the output of feedforward network is then added to the attention output(before ff_norm)\n",
        "    out = h + self.feedforward(self.ff_norm(h))\n",
        "    # Shape: [bsz,seq_len,dim]\n",
        "    return out\n",
        "\n",
        "\n",
        "### Test: TransformerBlock ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)\n",
        "transformer_block = TransformerBlock(ModelArgs)\n",
        "transformer_block_out = transformer_block(x,start_pos=0, inference=False)\n",
        "print(f\"transformer_block_out.shape: {transformer_block_out.shape}\")\n",
        "\"\"\"\n",
        "\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "transformer_block_out.shape: torch.Size([10, 64, 128])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bJBJZi2TuV-i",
        "outputId": "daa27fc4-a612-4493-9fc3-bea9f9d55480"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntransformer_block_out.shape: torch.Size([10, 64, 128])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step3: The Output Block\n",
        "# This is the Llama 3 model. Again, the class name is maintained as Transformer to match with Meta Llama 3 model.\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, params: ModelArgs):\n",
        "    super().__init__()\n",
        "    # set all the ModelArgs in params variable\n",
        "    self.params = params\n",
        "    # Initilizate embedding class from the input block\n",
        "    self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "\n",
        "    # Initialize the decoder block and store it inside the ModuleList.\n",
        "    # This is because we've 4 decoder blocks in our Llama 3 model. (Official Llama 3 has 32 blocks)\n",
        "    self.layers = nn.ModuleList()\n",
        "    for layer_id in range(params.n_layers):\n",
        "      self.layers.append(TransformerBlock(args=params))\n",
        "\n",
        "    # Initilizate RMSNorm for the output block\n",
        "    self.norm = RMSNorm(params.dim, eps = params.norm_eps)\n",
        "\n",
        "    # Initilizate linear layer at the output block.\n",
        "    self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x, start_pos=0, targets=None):\n",
        "\n",
        "    # start_pos = token position for inference mode, inference = True for inference and False for training mode\n",
        "    # x is the batch of token_ids generated from the texts or prompts using tokenizers.\n",
        "    # x[bsz, seq_len] -> h[bsz, seq_len, dim]\n",
        "    h = self.tok_embeddings(x)\n",
        "\n",
        "    # If the target is none, Inference mode is activated and set to \"True\" and \"False\" if Training mode is activated.\n",
        "    if targets is None:\n",
        "      inference = True\n",
        "    else:\n",
        "      inference = False\n",
        "\n",
        "    # The embeddings (h) will then pass though all the decoder blocks.\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, inference)\n",
        "\n",
        "    # The output from the final decoder block will feed into the RMSNorm\n",
        "    h = self.norm(h)\n",
        "\n",
        "    # After normalized, the embedding h will then feed into the Linear layer.\n",
        "    # The main task of the Linear layer is to generate logits that maps the embeddings with the vocabulary size.\n",
        "    # h[bsz, seq_len, dim] -> logits[bsz, seq_len, vocab_size]\n",
        "    logits = self.output(h).float()\n",
        "    loss = None\n",
        "\n",
        "    # Inference mode is activated if the targets is not available\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    # Training mode is activated if the targets are available. And Loss will be calculated for further model training.\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits.view(-1, self.params.vocab_size), targets.view(-1))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "### Test: Transformer (Llama Model) ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "model = Transformer(ModelArgs).to(ModelArgs.device)\n",
        "print(model)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WnbAh1YiuWAo",
        "outputId": "7f87f58a-8e42-4ffa-8a99-ef1ad52b4d52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = Transformer(ModelArgs).to(ModelArgs.device)\\nprint(model)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Train Llama 3Â Model:\n",
        "\n",
        "# Create a dataset by encoding the entire tiny_shakespeare data token_ids list using the tokenizer's encode function that we've built at the input block section\n",
        "dataset = torch.tensor(encode(data), dtype=torch.int).to(ModelArgs.device)\n",
        "print(f\"dataset-shape: {dataset.shape}\")\n",
        "\n",
        "# Define function to generate batches from the given dataset\n",
        "def get_dataset_batch(data, split, args:ModelArgs):\n",
        "  seq_len = args.max_seq_len\n",
        "  batch_size = args.max_batch_size\n",
        "  device = args.device\n",
        "\n",
        "  train = data[:int(0.8 * len(data))]\n",
        "  val = data[int(0.8 * len(data)): int(0.9 * len(data))]\n",
        "  test = data[int(0.9 * len(data)):]\n",
        "\n",
        "  batch_data = train\n",
        "  if split == \"val\":\n",
        "    batch_data = val\n",
        "\n",
        "  if split == \"test\":\n",
        "    batch_data = test\n",
        "\n",
        "  # Picking random starting points from the dataset to give random samples for training, validation and testing.\n",
        "\n",
        "  ix = torch.randint(0, len(batch_data) - seq_len - 3, (batch_size,)).to(device)\n",
        "  x = torch.stack([torch.cat([token_bos, batch_data[i:i+seq_len-1]]) for i in ix]).long().to(device)\n",
        "  y = torch.stack([torch.cat([batch_data[i+1:i+seq_len], token_eos]) for i in ix]).long().to(device)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "### Test: get_dataset function ###\n",
        "\"\"\"\n",
        "xs, ys = get_dataset_batch(dataset, split=\"train\", args=ModelArgs)\n",
        "print([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])\n",
        "\"\"\"\n",
        "\n",
        "# Define a evaluate loss function to calculate and store training and validation loss for logging and plotting\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, args:ModelArgs):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = []\n",
        "    for _ in range(10):\n",
        "      xb, yb = get_dataset_batch(dataset, split, args)\n",
        "      _, loss = model(x=xb, targets=yb)\n",
        "      losses.append(loss.item())\n",
        "    out[split] = np.mean(losses)\n",
        "\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "# Define a training function to perform model training\n",
        "def train(model, optimizer, args:ModelArgs):\n",
        "    epochs = args.epochs\n",
        "    log_interval = args.log_interval\n",
        "    device = args.device\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        xs, ys = get_dataset_batch(dataset, 'train', args)\n",
        "        xs = xs.to(device)\n",
        "        ys = ys.to(device)\n",
        "        logits, loss = model(x=xs, targets=ys)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % log_interval == 0:\n",
        "            batch_time = time.time() - start_time\n",
        "            x = evaluate_loss(model, args)\n",
        "            losses += [x]\n",
        "            print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "    # Print the final validation loss\n",
        "    print(\"validation loss: \", losses[-1]['val'])\n",
        "    # Display the interval losses in plot\n",
        "    return pd.DataFrame(losses).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHFdSdwJuWCu",
        "outputId": "82056097-119b-441b-ab91-f0c28955e144"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset-shape: torch.Size([1115394])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Start training our Llama 3 model\n",
        "model = Transformer(ModelArgs).to(ModelArgs.device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "train(model, optimizer, ModelArgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AfPVpUv0uWEz",
        "outputId": "0697c46c-a1eb-4e95-9165-d85f412fd848"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | val loss 3.946 | Time 1.547\n",
            "Epoch 10 | val loss 3.084 | Time 1.398\n",
            "Epoch 20 | val loss 2.992 | Time 1.421\n",
            "Epoch 30 | val loss 2.955 | Time 1.415\n",
            "Epoch 40 | val loss 2.961 | Time 1.423\n",
            "Epoch 50 | val loss 2.906 | Time 1.435\n",
            "Epoch 60 | val loss 2.889 | Time 1.458\n",
            "Epoch 70 | val loss 2.851 | Time 1.486\n",
            "Epoch 80 | val loss 2.849 | Time 1.481\n",
            "Epoch 90 | val loss 2.831 | Time 1.493\n",
            "Epoch 100 | val loss 2.824 | Time 1.509\n",
            "Epoch 110 | val loss 2.805 | Time 1.535\n",
            "Epoch 120 | val loss 2.766 | Time 1.527\n",
            "Epoch 130 | val loss 2.785 | Time 1.537\n",
            "Epoch 140 | val loss 2.739 | Time 1.532\n",
            "Epoch 150 | val loss 2.705 | Time 1.529\n",
            "Epoch 160 | val loss 2.666 | Time 1.513\n",
            "Epoch 170 | val loss 2.688 | Time 1.495\n",
            "Epoch 180 | val loss 2.645 | Time 1.483\n",
            "Epoch 190 | val loss 2.633 | Time 1.475\n",
            "Epoch 200 | val loss 2.609 | Time 1.477\n",
            "Epoch 210 | val loss 2.611 | Time 1.457\n",
            "Epoch 220 | val loss 2.587 | Time 1.453\n",
            "Epoch 230 | val loss 2.593 | Time 1.446\n",
            "Epoch 240 | val loss 2.586 | Time 1.443\n",
            "Epoch 250 | val loss 2.584 | Time 1.455\n",
            "Epoch 260 | val loss 2.576 | Time 1.440\n",
            "Epoch 270 | val loss 2.602 | Time 1.436\n",
            "Epoch 280 | val loss 2.594 | Time 1.434\n",
            "Epoch 290 | val loss 2.578 | Time 1.452\n",
            "Epoch 300 | val loss 2.543 | Time 1.442\n",
            "Epoch 310 | val loss 2.519 | Time 1.443\n",
            "Epoch 320 | val loss 2.496 | Time 1.446\n",
            "Epoch 330 | val loss 2.502 | Time 1.455\n",
            "Epoch 340 | val loss 2.511 | Time 1.463\n",
            "Epoch 350 | val loss 2.459 | Time 1.460\n",
            "Epoch 360 | val loss 2.507 | Time 1.465\n",
            "Epoch 370 | val loss 2.479 | Time 1.471\n",
            "Epoch 380 | val loss 2.483 | Time 1.472\n",
            "Epoch 390 | val loss 2.480 | Time 1.487\n",
            "Epoch 400 | val loss 2.483 | Time 1.481\n",
            "Epoch 410 | val loss 2.457 | Time 1.479\n",
            "Epoch 420 | val loss 2.445 | Time 1.478\n",
            "Epoch 430 | val loss 2.479 | Time 1.485\n",
            "Epoch 440 | val loss 2.433 | Time 1.475\n",
            "Epoch 450 | val loss 2.467 | Time 1.470\n",
            "Epoch 460 | val loss 2.436 | Time 1.467\n",
            "Epoch 470 | val loss 2.434 | Time 1.471\n",
            "Epoch 480 | val loss 2.396 | Time 1.464\n",
            "Epoch 490 | val loss 2.423 | Time 1.462\n",
            "Epoch 500 | val loss 2.439 | Time 1.456\n",
            "Epoch 510 | val loss 2.441 | Time 1.456\n",
            "Epoch 520 | val loss 2.386 | Time 1.464\n",
            "Epoch 530 | val loss 2.369 | Time 1.451\n",
            "Epoch 540 | val loss 2.435 | Time 1.450\n",
            "Epoch 550 | val loss 2.389 | Time 1.452\n",
            "Epoch 560 | val loss 2.379 | Time 1.448\n",
            "Epoch 570 | val loss 2.383 | Time 1.489\n",
            "Epoch 580 | val loss 2.351 | Time 1.452\n",
            "Epoch 590 | val loss 2.372 | Time 1.451\n",
            "Epoch 600 | val loss 2.394 | Time 1.466\n",
            "Epoch 610 | val loss 2.349 | Time 1.475\n",
            "Epoch 620 | val loss 2.366 | Time 1.462\n",
            "Epoch 630 | val loss 2.343 | Time 1.465\n",
            "Epoch 640 | val loss 2.352 | Time 1.459\n",
            "Epoch 650 | val loss 2.353 | Time 1.470\n",
            "Epoch 660 | val loss 2.357 | Time 1.488\n",
            "Epoch 670 | val loss 2.370 | Time 1.462\n",
            "Epoch 680 | val loss 2.357 | Time 1.478\n",
            "Epoch 690 | val loss 2.337 | Time 1.471\n",
            "Epoch 700 | val loss 2.357 | Time 1.486\n",
            "Epoch 710 | val loss 2.334 | Time 1.470\n",
            "Epoch 720 | val loss 2.366 | Time 1.467\n",
            "Epoch 730 | val loss 2.352 | Time 1.473\n",
            "Epoch 740 | val loss 2.374 | Time 1.465\n",
            "Epoch 750 | val loss 2.335 | Time 1.487\n",
            "Epoch 760 | val loss 2.320 | Time 1.460\n",
            "Epoch 770 | val loss 2.348 | Time 1.466\n",
            "Epoch 780 | val loss 2.354 | Time 1.460\n",
            "Epoch 790 | val loss 2.302 | Time 1.479\n",
            "Epoch 800 | val loss 2.311 | Time 1.458\n",
            "Epoch 810 | val loss 2.327 | Time 1.458\n",
            "Epoch 820 | val loss 2.341 | Time 1.457\n",
            "Epoch 830 | val loss 2.327 | Time 1.451\n",
            "Epoch 840 | val loss 2.345 | Time 1.474\n",
            "Epoch 850 | val loss 2.296 | Time 1.453\n",
            "Epoch 860 | val loss 2.318 | Time 1.454\n",
            "Epoch 870 | val loss 2.297 | Time 1.451\n",
            "Epoch 880 | val loss 2.309 | Time 1.448\n",
            "Epoch 890 | val loss 2.314 | Time 1.451\n",
            "Epoch 900 | val loss 2.304 | Time 1.453\n",
            "Epoch 910 | val loss 2.312 | Time 1.457\n",
            "Epoch 920 | val loss 2.264 | Time 1.459\n",
            "Epoch 930 | val loss 2.262 | Time 1.476\n",
            "Epoch 940 | val loss 2.320 | Time 1.456\n",
            "Epoch 950 | val loss 2.285 | Time 1.459\n",
            "Epoch 960 | val loss 2.279 | Time 1.468\n",
            "Epoch 970 | val loss 2.320 | Time 1.459\n",
            "Epoch 980 | val loss 2.249 | Time 1.485\n",
            "Epoch 990 | val loss 2.254 | Time 1.462\n",
            "Epoch 1000 | val loss 2.257 | Time 1.469\n",
            "Epoch 1010 | val loss 2.265 | Time 1.464\n",
            "Epoch 1020 | val loss 2.264 | Time 1.486\n",
            "Epoch 1030 | val loss 2.251 | Time 1.468\n",
            "Epoch 1040 | val loss 2.309 | Time 1.471\n",
            "Epoch 1050 | val loss 2.276 | Time 1.470\n",
            "Epoch 1060 | val loss 2.305 | Time 1.472\n",
            "Epoch 1070 | val loss 2.263 | Time 1.488\n",
            "Epoch 1080 | val loss 2.243 | Time 1.470\n",
            "Epoch 1090 | val loss 2.235 | Time 1.460\n",
            "Epoch 1100 | val loss 2.222 | Time 1.462\n",
            "Epoch 1110 | val loss 2.277 | Time 1.463\n",
            "Epoch 1120 | val loss 2.280 | Time 1.460\n",
            "Epoch 1130 | val loss 2.242 | Time 1.459\n",
            "Epoch 1140 | val loss 2.254 | Time 1.456\n",
            "Epoch 1150 | val loss 2.241 | Time 1.460\n",
            "Epoch 1160 | val loss 2.220 | Time 1.483\n",
            "Epoch 1170 | val loss 2.235 | Time 1.462\n",
            "Epoch 1180 | val loss 2.212 | Time 1.470\n",
            "Epoch 1190 | val loss 2.271 | Time 1.457\n",
            "Epoch 1200 | val loss 2.218 | Time 1.467\n",
            "Epoch 1210 | val loss 2.196 | Time 1.464\n",
            "Epoch 1220 | val loss 2.216 | Time 1.468\n",
            "Epoch 1230 | val loss 2.254 | Time 1.468\n",
            "Epoch 1240 | val loss 2.257 | Time 1.467\n",
            "Epoch 1250 | val loss 2.226 | Time 1.487\n",
            "Epoch 1260 | val loss 2.240 | Time 1.478\n",
            "Epoch 1270 | val loss 2.204 | Time 1.468\n",
            "Epoch 1280 | val loss 2.196 | Time 1.465\n",
            "Epoch 1290 | val loss 2.226 | Time 1.482\n",
            "Epoch 1300 | val loss 2.276 | Time 1.463\n",
            "Epoch 1310 | val loss 2.246 | Time 1.466\n",
            "Epoch 1320 | val loss 2.220 | Time 1.476\n",
            "Epoch 1330 | val loss 2.225 | Time 1.470\n",
            "Epoch 1340 | val loss 2.200 | Time 1.479\n",
            "Epoch 1350 | val loss 2.180 | Time 1.474\n",
            "Epoch 1360 | val loss 2.253 | Time 1.463\n",
            "Epoch 1370 | val loss 2.256 | Time 1.464\n",
            "Epoch 1380 | val loss 2.223 | Time 1.491\n",
            "Epoch 1390 | val loss 2.265 | Time 1.467\n",
            "Epoch 1400 | val loss 2.209 | Time 1.497\n",
            "Epoch 1410 | val loss 2.230 | Time 1.467\n",
            "Epoch 1420 | val loss 2.274 | Time 1.471\n",
            "Epoch 1430 | val loss 2.230 | Time 1.481\n",
            "Epoch 1440 | val loss 2.249 | Time 1.458\n",
            "Epoch 1450 | val loss 2.215 | Time 1.460\n",
            "Epoch 1460 | val loss 2.229 | Time 1.459\n",
            "Epoch 1470 | val loss 2.180 | Time 1.461\n",
            "Epoch 1480 | val loss 2.164 | Time 1.463\n",
            "Epoch 1490 | val loss 2.196 | Time 1.463\n",
            "Epoch 1500 | val loss 2.280 | Time 1.461\n",
            "Epoch 1510 | val loss 2.255 | Time 1.464\n",
            "Epoch 1520 | val loss 2.199 | Time 1.477\n",
            "Epoch 1530 | val loss 2.218 | Time 1.462\n",
            "Epoch 1540 | val loss 2.223 | Time 1.463\n",
            "Epoch 1550 | val loss 2.195 | Time 1.460\n",
            "Epoch 1560 | val loss 2.212 | Time 1.460\n",
            "Epoch 1570 | val loss 2.192 | Time 1.460\n",
            "Epoch 1580 | val loss 2.228 | Time 1.462\n",
            "Epoch 1590 | val loss 2.182 | Time 1.464\n",
            "Epoch 1600 | val loss 2.225 | Time 1.456\n",
            "Epoch 1610 | val loss 2.234 | Time 1.469\n",
            "Epoch 1620 | val loss 2.198 | Time 1.457\n",
            "Epoch 1630 | val loss 2.177 | Time 1.457\n",
            "Epoch 1640 | val loss 2.221 | Time 1.470\n",
            "Epoch 1650 | val loss 2.216 | Time 1.457\n",
            "Epoch 1660 | val loss 2.202 | Time 1.467\n",
            "Epoch 1670 | val loss 2.164 | Time 1.457\n",
            "Epoch 1680 | val loss 2.224 | Time 1.457\n",
            "Epoch 1690 | val loss 2.175 | Time 1.455\n",
            "Epoch 1700 | val loss 2.179 | Time 1.467\n",
            "Epoch 1710 | val loss 2.232 | Time 1.462\n",
            "Epoch 1720 | val loss 2.222 | Time 1.457\n",
            "Epoch 1730 | val loss 2.210 | Time 1.463\n",
            "Epoch 1740 | val loss 2.194 | Time 1.454\n",
            "Epoch 1750 | val loss 2.177 | Time 1.477\n",
            "Epoch 1760 | val loss 2.202 | Time 1.460\n",
            "Epoch 1770 | val loss 2.197 | Time 1.457\n",
            "Epoch 1780 | val loss 2.214 | Time 1.458\n",
            "Epoch 1790 | val loss 2.192 | Time 1.469\n",
            "Epoch 1800 | val loss 2.182 | Time 1.458\n",
            "Epoch 1810 | val loss 2.197 | Time 1.461\n",
            "Epoch 1820 | val loss 2.163 | Time 1.452\n",
            "Epoch 1830 | val loss 2.206 | Time 1.456\n",
            "Epoch 1840 | val loss 2.180 | Time 1.473\n",
            "Epoch 1850 | val loss 2.209 | Time 1.460\n",
            "Epoch 1860 | val loss 2.180 | Time 1.461\n",
            "Epoch 1870 | val loss 2.161 | Time 1.458\n",
            "Epoch 1880 | val loss 2.187 | Time 1.471\n",
            "Epoch 1890 | val loss 2.210 | Time 1.457\n",
            "Epoch 1900 | val loss 2.230 | Time 1.460\n",
            "Epoch 1910 | val loss 2.161 | Time 1.461\n",
            "Epoch 1920 | val loss 2.188 | Time 1.457\n",
            "Epoch 1930 | val loss 2.194 | Time 1.478\n",
            "Epoch 1940 | val loss 2.191 | Time 1.463\n",
            "Epoch 1950 | val loss 2.176 | Time 1.456\n",
            "Epoch 1960 | val loss 2.195 | Time 1.453\n",
            "Epoch 1970 | val loss 2.195 | Time 1.477\n",
            "Epoch 1980 | val loss 2.149 | Time 1.455\n",
            "Epoch 1990 | val loss 2.192 | Time 1.455\n",
            "Epoch 2000 | val loss 2.194 | Time 1.456\n",
            "Epoch 2010 | val loss 2.198 | Time 1.460\n",
            "Epoch 2020 | val loss 2.221 | Time 1.471\n",
            "Epoch 2030 | val loss 2.182 | Time 1.458\n",
            "Epoch 2040 | val loss 2.282 | Time 1.456\n",
            "Epoch 2050 | val loss 2.211 | Time 1.454\n",
            "Epoch 2060 | val loss 2.200 | Time 1.462\n",
            "Epoch 2070 | val loss 2.254 | Time 1.459\n",
            "Epoch 2080 | val loss 2.203 | Time 1.457\n",
            "Epoch 2090 | val loss 2.212 | Time 1.458\n",
            "Epoch 2100 | val loss 2.240 | Time 1.454\n",
            "Epoch 2110 | val loss 2.197 | Time 1.464\n",
            "Epoch 2120 | val loss 2.207 | Time 1.459\n",
            "Epoch 2130 | val loss 2.210 | Time 1.452\n",
            "Epoch 2140 | val loss 2.221 | Time 1.455\n",
            "Epoch 2150 | val loss 2.196 | Time 1.451\n",
            "Epoch 2160 | val loss 2.202 | Time 1.482\n",
            "Epoch 2170 | val loss 2.168 | Time 1.462\n",
            "Epoch 2180 | val loss 2.169 | Time 1.454\n",
            "Epoch 2190 | val loss 2.145 | Time 1.460\n",
            "Epoch 2200 | val loss 2.177 | Time 1.467\n",
            "Epoch 2210 | val loss 2.214 | Time 1.458\n",
            "Epoch 2220 | val loss 2.178 | Time 1.455\n",
            "Epoch 2230 | val loss 2.212 | Time 1.450\n",
            "Epoch 2240 | val loss 2.200 | Time 1.459\n",
            "Epoch 2250 | val loss 2.179 | Time 1.473\n",
            "Epoch 2260 | val loss 2.232 | Time 1.452\n",
            "Epoch 2270 | val loss 2.162 | Time 1.463\n",
            "Epoch 2280 | val loss 2.168 | Time 1.456\n",
            "Epoch 2290 | val loss 2.137 | Time 1.460\n",
            "Epoch 2300 | val loss 2.204 | Time 1.457\n",
            "Epoch 2310 | val loss 2.192 | Time 1.457\n",
            "Epoch 2320 | val loss 2.175 | Time 1.455\n",
            "Epoch 2330 | val loss 2.172 | Time 1.458\n",
            "Epoch 2340 | val loss 2.221 | Time 1.479\n",
            "Epoch 2350 | val loss 2.197 | Time 1.455\n",
            "Epoch 2360 | val loss 2.147 | Time 1.451\n",
            "Epoch 2370 | val loss 2.172 | Time 1.455\n",
            "Epoch 2380 | val loss 2.146 | Time 1.477\n",
            "Epoch 2390 | val loss 2.189 | Time 1.454\n",
            "Epoch 2400 | val loss 2.213 | Time 1.457\n",
            "Epoch 2410 | val loss 2.117 | Time 1.457\n",
            "Epoch 2420 | val loss 2.145 | Time 1.455\n",
            "Epoch 2430 | val loss 2.185 | Time 1.471\n",
            "Epoch 2440 | val loss 2.159 | Time 1.458\n",
            "Epoch 2450 | val loss 2.167 | Time 1.454\n",
            "Epoch 2460 | val loss 2.141 | Time 1.453\n",
            "Epoch 2470 | val loss 2.235 | Time 1.455\n",
            "Epoch 2480 | val loss 2.141 | Time 1.457\n",
            "Epoch 2490 | val loss 2.133 | Time 1.453\n",
            "validation loss:  2.1331812381744384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Inference Llama 3Â Model:\n",
        "# This function generates text sequences based on provided prompts using the LLama 3 model we've built and trained.\n",
        "\n",
        "def generate(model, prompts: str, params: ModelArgs, max_gen_len: int=500, temperature: float = 0.6, top_p: float = 0.9):\n",
        "\n",
        "    # prompt_tokens: List of user input texts or prompts\n",
        "    # max_gen_len: Maximum length of the generated text sequence.\n",
        "    # temperature: Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
        "    # top_p: Top-p probability threshold for sampling prob output from the logits. Defaults to 0.9.\n",
        "    # prompt_tokens = [0]\n",
        "    bsz = 1  #For inferencing, in general user just input one prompt which we'll take it as 1-batch\n",
        "    prompt_tokens = token_bos.tolist() + encode(prompts)\n",
        "    assert len(prompt_tokens) <= params.max_seq_len, \"prompt token length should be small than max_seq_len\"\n",
        "    total_len = min(len(prompt_tokens)+max_gen_len, params.max_seq_len)\n",
        "\n",
        "    # this tokens matrix is to store the input prompts and all the output that is generated by model.\n",
        "    # later we'll use the tokenizers decode function to decode this token to view results in text format\n",
        "    tokens = torch.full((bsz,total_len), fill_value=token_pad.item(), dtype=torch.long, device=params.device)\n",
        "\n",
        "    # fill in the prompt tokens into the token matrix\n",
        "    tokens[:,:len(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=params.device)\n",
        "\n",
        "    #create a prompt_mask_token for later use to identify if the token is a prompt token or a padding token\n",
        "    # True if it is a prompt token, False if it is a padding token\n",
        "    input_text_mask = tokens != token_pad.item()\n",
        "\n",
        "    #now we can start inferencing using one token at a time from the prompt_tokens list starting with the first position.\n",
        "    prev_pos = 0\n",
        "    for cur_pos in range(1, total_len):\n",
        "      with torch.no_grad():\n",
        "        logits, _ = model(x=tokens[:,prev_pos:cur_pos], start_pos=prev_pos)\n",
        "      if temperature > 0:\n",
        "        probs = torch.softmax(logits[:, -1]/temperature, dim=-1)\n",
        "        next_token = sample_top_p(probs, top_p)\n",
        "      else:\n",
        "        next_token = torch.argmax(logits[:, -1], dim=-1)\n",
        "\n",
        "      next_token = next_token.reshape(-1)\n",
        "\n",
        "      # only replace the token if it's a padding token\n",
        "      next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "      tokens[:, cur_pos] = next_token\n",
        "\n",
        "      prev_pos = cur_pos\n",
        "      if tokens[:,cur_pos]==token_pad.item() and next_token == token_eos.item():\n",
        "        break\n",
        "\n",
        "    output_tokens, output_texts = [], []\n",
        "\n",
        "    for i, toks in enumerate(tokens.tolist()):\n",
        "      # eos_idx = toks.index(token_eos.item())\n",
        "      if token_eos.item() in toks:\n",
        "        eos_idx = toks.index(token_eos.item())\n",
        "        toks = toks[:eos_idx]\n",
        "\n",
        "      output_tokens.append(toks)\n",
        "      output_texts.append(decode(toks))\n",
        "    return output_tokens, output_texts\n",
        "\n",
        "# Perform top-p (nucleus) sampling on a probability distribution.\n",
        "# probs (torch.Tensor): Probability distribution tensor derived from the logits.\n",
        "# p: Probability threshold for top-p sampling.\n",
        "# According to the paper, Top-p sampling selects the smallest set of tokens whose cumulative probability mass exceeds the threshold p.\n",
        "# The distribution is renormalized based on the selected tokens.\n",
        "def sample_top_p(probs, p):\n",
        "    probs_sort, prob_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(prob_idx, -1, next_token)\n",
        "    # Sampled token indices from the vocabular is returned\n",
        "    return next_token"
      ],
      "metadata": {
        "id": "zhEJofRiuWHJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Perform the inferencing on user input prompts\n",
        "prompts = \"Consider you what services he has done\"\n",
        "output_tokens, output_texts = generate(model, prompts, ModelArgs)\n",
        "output_texts = output_texts[0].replace(\"<|begin_of_text|>\", \"\")\n",
        "print(output_texts)\n",
        "\n",
        "## Output ##\n",
        "\"\"\"\n",
        "Consider you what services he has done o eretrane\n",
        "adetranytnn i eey i ade hs rcuh i eey,ad hsatsTns rpae,T\n",
        "eon o i hseflns o i eee ee hs ote i ocal ersl,Bnnlnface\n",
        "o i hmr a il nwye ademto nt i a ere\n",
        "h i ees.\n",
        "Frm oe o etrane o oregae,alh,t orede i oeral\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jwC6wxrSuWI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29OxDHZYuWLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_n0ZopzuWNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cPk145spuWPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aV_4HctuWRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyAjDEcquWTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}